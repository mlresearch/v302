---
title: 'Zero-Shot LLM Generation of Energy Notifications for African Languages: A
  Benchmark Study'
abstract: Large Language Models (LLMs) have demonstrated significant advancements
  in natural language applications but often exhibit performance disparities for low-resource
  languages, particularly African languages underrepresented in training corpora.
  This paper addresses this gap by evaluating the zero-shot text generation capabilities
  of LLMs within the energy domain for six widely spoken African languages. We introduce
  a novel multilingual benchmark dataset of energy management notifications and use
  it to assess four recent open-source LLMs (1B-7B parameters). Employing a zero-shot
  learning approach with multiple prompts and established NLP metrics (Statistics-based,
  Model-based, Perplexity) without fine-tuning, our findings reveal varying model
  strengths across languages and metrics. For instance, while some models excel in
  content overlap (ROUGE) for languages like English and French, others show better
  fluency (Perplexity) or semantic similarity (BERTScore), with performance shifting
  notably for African languages.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: haddad26a
month: 0
tex_title: 'Zero-Shot LLM Generation of Energy Notifications for African Languages:
  A Benchmark Study'
firstpage: 1
lastpage: 10
page: 1-10
order: 1
cycles: false
bibtex_author: Haddad, Hatem and Jerbi, Feres and Smaali, Issam
author:
- given: Hatem
  family: Haddad
- given: Feres
  family: Jerbi
- given: Issam
  family: Smaali
date: 2026-02-13
address:
container-title: DLI 2025 Research Track
volume: '302'
genre: inproceedings
issued:
  date-parts:
  - 2026
  - 2
  - 13
pdf: https://raw.githubusercontent.com/mlresearch/v302/main/assets/haddad26a/haddad26a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
