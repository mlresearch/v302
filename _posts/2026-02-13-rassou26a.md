---
title: Combatting Language Forgetting in Low-Resourced Settings
abstract: 'Neural machine translation becomes a continual learning challenge as language
  evolves over time. While Transformer-based models excel at capturing linguistic
  patterns from large corpora, they require continual updates to adapt to new language
  use without losing previously acquired knowledge. In this work, we introduce Latent
  Replay Buffers to the NLP domain for the first time by implementing and fine-tuning
  our Latent Replay Transformer. We conduct initial experiments for low-resource languages
  on Small-100, a distilled version of a multilingual transformer trained on 100 languages,
  to be well-suited for deployment in memory- and data-constrained environments. Our
  findings reveal an intriguing trade-off in the selection of latent activations to
  store for effective replay. We release our code to support both the Continual Learning
  and NLP for Low-Resourced Languages communities. <b>Keywords</b>: Continual Learning,
  Transformers, Low-Resourced Languages.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rassou26a
month: 0
tex_title: Combatting Language Forgetting in Low-Resourced Settings
firstpage: 1
lastpage: 11
page: 1-11
order: 1
cycles: false
bibtex_author: Rassou, Emmanuel
author:
- given: Emmanuel
  family: Rassou
date: 2026-02-13
address:
container-title: DLI 2025 Research Track
volume: '302'
genre: inproceedings
issued:
  date-parts:
  - 2026
  - 2
  - 13
pdf: https://raw.githubusercontent.com/mlresearch/v302/main/assets/rassou26a/rassou26a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
